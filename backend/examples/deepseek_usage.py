#!/usr/bin/env python3
"""
DeepSeek ä½¿ç”¨ç¤ºä¾‹

æ­¤æ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•åœ¨é¡¹ç›®ä¸­ä½¿ç”¨ DeepSeek æ¨¡å‹çš„å„ç§åŠŸèƒ½ã€‚
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.llm import make_llm_api_call

async def example_basic_chat():
    """åŸºæœ¬èŠå¤©ç¤ºä¾‹"""
    print("=== åŸºæœ¬èŠå¤©ç¤ºä¾‹ ===")
    
    response = await make_llm_api_call(
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ DeepSeek æ¨¡å‹çš„ç‰¹ç‚¹"}
        ],
        model_name="deepseek/deepseek-chat",
        temperature=0.7
    )
    
    print(f"ç”¨æˆ·: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ DeepSeek æ¨¡å‹çš„ç‰¹ç‚¹")
    print(f"DeepSeek: {response.choices[0].message.content}")
    print()

async def example_code_generation():
    """ä»£ç ç”Ÿæˆç¤ºä¾‹"""
    print("=== ä»£ç ç”Ÿæˆç¤ºä¾‹ ===")
    
    response = await make_llm_api_call(
        messages=[
            {"role": "user", "content": "è¯·ç”¨ Python å†™ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬ n é¡¹"}
        ],
        model_name="deepseek/deepseek-coder",
        temperature=0.3
    )
    
    print(f"ç”¨æˆ·: è¯·ç”¨ Python å†™ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬ n é¡¹")
    print(f"DeepSeek: {response.choices[0].message.content}")
    print()

async def example_reasoning():
    """æ¨ç†ç¤ºä¾‹"""
    print("=== æ¨ç†ç¤ºä¾‹ ===")
    
    response = await make_llm_api_call(
        messages=[
            {"role": "user", "content": "ä¸€ä¸ªå•†åº—æœ‰ 100 ä¸ªè‹¹æœï¼Œç¬¬ä¸€å¤©å–å‡º 20%ï¼Œç¬¬äºŒå¤©å–å‡ºå‰©ä¸‹çš„ 30%ï¼Œç¬¬ä¸‰å¤©å–å‡ºå‰©ä¸‹çš„ 50%ï¼Œé—®æœ€åè¿˜å‰©å¤šå°‘ä¸ªè‹¹æœï¼Ÿè¯·è¯¦ç»†è®¡ç®—ã€‚"}
        ],
        model_name="deepseek/deepseek-reasoner",
        temperature=0.1
    )
    
    print(f"ç”¨æˆ·: ä¸€ä¸ªå•†åº—æœ‰ 100 ä¸ªè‹¹æœï¼Œç¬¬ä¸€å¤©å–å‡º 20%ï¼Œç¬¬äºŒå¤©å–å‡ºå‰©ä¸‹çš„ 30%ï¼Œç¬¬ä¸‰å¤©å–å‡ºå‰©ä¸‹çš„ 50%ï¼Œé—®æœ€åè¿˜å‰©å¤šå°‘ä¸ªè‹¹æœï¼Ÿè¯·è¯¦ç»†è®¡ç®—ã€‚")
    print(f"DeepSeek: {response.choices[0].message.content}")
    print()

async def example_streaming():
    """æµå¼å“åº”ç¤ºä¾‹"""
    print("=== æµå¼å“åº”ç¤ºä¾‹ ===")
    
    print("ç”¨æˆ·: è¯·å†™ä¸€é¦–å…³äºç§‹å¤©çš„è¯—")
    print("DeepSeek: ", end="", flush=True)
    
    response = await make_llm_api_call(
        messages=[
            {"role": "user", "content": "è¯·å†™ä¸€é¦–å…³äºç§‹å¤©çš„è¯—"}
        ],
        model_name="deepseek/deepseek-chat",
        temperature=0.8,
        stream=True
    )
    
    async for chunk in response:
        if hasattr(chunk, 'choices') and chunk.choices:
            content = chunk.choices[0].delta.content
            if content:
                print(content, end='', flush=True)
    print("\n")

async def example_tool_calling():
    """å·¥å…·è°ƒç”¨ç¤ºä¾‹"""
    print("=== å·¥å…·è°ƒç”¨ç¤ºä¾‹ ===")
    
    tools = [
        {
            "type": "function",
            "function": {
                "name": "calculate",
                "description": "æ‰§è¡Œæ•°å­¦è®¡ç®—",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "expression": {
                            "type": "string",
                            "description": "æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ '2 + 3 * 4'"
                        }
                    },
                    "required": ["expression"]
                }
            }
        }
    ]
    
    response = await make_llm_api_call(
        messages=[
            {"role": "user", "content": "è¯·è®¡ç®— 15 çš„å¹³æ–¹æ ¹"}
        ],
        model_name="deepseek/deepseek-chat",
        temperature=0.1,
        tools=tools,
        tool_choice="auto"
    )
    
    print(f"ç”¨æˆ·: è¯·è®¡ç®— 15 çš„å¹³æ–¹æ ¹")
    print(f"DeepSeek: {response.choices[0].message}")
    print()

async def example_multi_turn_conversation():
    """å¤šè½®å¯¹è¯ç¤ºä¾‹"""
    print("=== å¤šè½®å¯¹è¯ç¤ºä¾‹ ===")
    
    messages = [
        {"role": "user", "content": "æˆ‘æƒ³å­¦ä¹  Python ç¼–ç¨‹ï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ"}
    ]
    
    # ç¬¬ä¸€è½®å¯¹è¯
    response = await make_llm_api_call(
        messages=messages,
        model_name="deepseek/deepseek-chat",
        temperature=0.7
    )
    
    print(f"ç”¨æˆ·: {messages[0]['content']}")
    print(f"DeepSeek: {response.choices[0].message.content}")
    print()
    
    # æ·»åŠ å›å¤åˆ°å¯¹è¯å†å²
    messages.append({"role": "assistant", "content": response.choices[0].message.content})
    messages.append({"role": "user", "content": "é‚£ä½ èƒ½æ¨èä¸€äº›å…·ä½“çš„ Python æ•™ç¨‹èµ„æºå—ï¼Ÿ"})
    
    # ç¬¬äºŒè½®å¯¹è¯
    response = await make_llm_api_call(
        messages=messages,
        model_name="deepseek/deepseek-chat",
        temperature=0.7
    )
    
    print(f"ç”¨æˆ·: é‚£ä½ èƒ½æ¨èä¸€äº›å…·ä½“çš„ Python æ•™ç¨‹èµ„æºå—ï¼Ÿ")
    print(f"DeepSeek: {response.choices[0].message.content}")
    print()

async def example_openrouter_deepseek():
    """é€šè¿‡ OpenRouter ä½¿ç”¨ DeepSeek ç¤ºä¾‹"""
    print("=== OpenRouter DeepSeek ç¤ºä¾‹ ===")
    
    response = await make_llm_api_call(
        messages=[
            {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}
        ],
        model_name="openrouter/deepseek/deepseek-chat",
        temperature=0.5
    )
    
    print(f"ç”¨æˆ·: è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ")
    print(f"DeepSeek (via OpenRouter): {response.choices[0].message.content}")
    print()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ DeepSeek ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("DEEPSEEK_API_KEY"):
        print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡åé‡è¯•:")
        print("export DEEPSEEK_API_KEY=your_api_key_here")
        return
    
    try:
        # è¿è¡Œå„ç§ç¤ºä¾‹
        await example_basic_chat()
        await example_code_generation()
        await example_reasoning()
        await example_streaming()
        await example_tool_calling()
        await example_multi_turn_conversation()
        await example_openrouter_deepseek()
        
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œç¤ºä¾‹æ—¶å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
